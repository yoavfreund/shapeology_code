import os
import numpy as np
import pickle as pk
import numpy as np
from glob import glob
from time import time, asctime
import sys
sys.path.append('../scripts/')
from lib.utils import configuration, setup_download_from_s3
from lib.permute import permutator

time_log=[]
def clock(message):
    print('%8.1f \t%s'%(time(),message))
    time_log.append((time(),message))

class Sorter:
    """Process the patch files generated by extractPatches.py and prepare
    them for analysis by Kmeans and diffusion-maps"""
    def __init__(self, K=20, src_root='/tmp'):
        '''
        Initialize a files sorter.
        :param K: number of final files
        :param src_root: directory for source files without permutation
        '''
        self.src_root = src_root
        assert os.path.exists(self.src_root)
        self.K = K

    def sort_file(self, size, M=100000, stem='permuted'):
        '''
        Create K files to collect cells from each section based on the proportion of the number of cells in each section to the total.
        One file contains 100000 cells.
        :param size: size of patches
        :param stem: name stem of directory to store the final results
        :return:
        '''
        # V = pk.load(open(pkl_file, 'rb'))
        self.size = size
        # self.dir = self.src_root + 'cells-'+str(size)+'/'
        self.saveDir = stem + '-' + str(size) + '/'
        if not os.path.exists(self.saveDir):
            os.makedirs(self.saveDir)
        self.fp = []
        for i in range(self.K):
            self.fp.append(open(self.saveDir + '/permuted-' + str(i) + '.bin', 'bw'))
        total = sum([os.path.getsize(fn) for fn in glob(self.src_root+'/*/'+str(size)+'.bin')])
        for fn in glob(self.src_root+'/*/'+str(size)+'.bin'):
            number = int(os.path.getsize(fn)/total*M)
            V = np.fromfile(fn, np.float16)
            V = V.reshape([-1,self.size,self.size])
            for i in range(self.K):
                self.fp[i].write(V[i*number:(i+1)*number, :, :])
        clock('Sort files of size ' + str(size))

    def close(self):
        '''
        Read and permute each file randomly collecting patches to achieve a random permutation of patches.
        :return:
        '''
        for i in range(self.K):
            self.fp[i].close()
            self.fp[i]=self.saveDir + '/permuted-' + str(i) + '.bin'

        for filename in self.fp:
            id = filename[filename.rfind('-') + 1:filename.rfind('.bin')]
            # file = open(filename, 'br')
            # D = file.read()
            # print('buffer length',len(D))
            # D = np.frombuffer(D, dtype=np.byte)
            # print(D.shape,type(D[0]))
            D = np.fromfile(filename, np.float16)
            elements = D.reshape([-1, self.size**2])
            # print(elements.shape)

            L = elements.shape[0]
            _order = np.random.permutation(L)
            permuted_elements = elements[_order, :]
            permuted_elements.tofile(filename)
            clock(id + ' files finished')
        clock('Permute files of size ' + str(self.size))


if __name__=='__main__':
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--file_num", type=int, default=20,
                        help="Number of permuted files to generate")
    parser.add_argument("--cell_num", type=int, default=100000,
                        help="Number of cells in one permuted file")
    parser.add_argument("--src_root", type=str, default=os.path.join(os.environ['ROOT_DIR'],'cells/'),
                        help="Path to directory containing cell files")
    parser.add_argument("--stem", type=str, default='permute/permuted',
                        help="Stem of filename of permuted files")
    args = parser.parse_args()
    K = args.file_num
    M = args.cell_num
    src_dir = args.src_root
    stem = args.stem

    yamlfile = os.environ['REPO_DIR'] + 'shape_params.yaml'
    params = configuration(yamlfile).getParams()
    root_dir = os.environ['ROOT_DIR']

    clock('Process Begin')
    t0 = time()
    setup_download_from_s3(src_dir[len(root_dir):])
    clock('Download From S3')
    sorter = Sorter(src_root=src_dir,K=K)
    size_thresholds = params['normalization']['size_thresholds']
    for size in size_thresholds:
        sorter.sort_file(size, M=M, stem=os.path.join(root_dir, stem))
        sorter.close()
        clock('Complete files of size '+str(size))
        print('Complete files of size '+str(size), time() - t0, 'seconds')
    log_fp = os.path.join(root_dir, 'TimeLog/')
    if not os.path.exists(log_fp):
        os.mkdir(log_fp)
    pk.dump(time_log,open(log_fp+'Time_log_permute_'+asctime()+'.pkl','wb'))